# Plan: Implement Severity Tracking System

## Branch
`feature/claude-gvl-severity-tracking`

## Overview
Add a metrics collection system where the reviewer agent emits JSON payloads after each review, tracking severity counts (Critical/Major/Minor) and category breakdowns (testing, ui_ux, business_logic, etc.). Data is stored as append-only bd comments on a dedicated `.claude-metrics` task for concurrent-safe writes and future analysis.

## Key patterns to follow
- `/workspace/agents/reviewer.md` lines 78-86 - Existing severity triage definitions (Critical/Major/Minor) that need to be tracked
- `/workspace/agents/reviewer.md` lines 100-114 - Existing bd comment pattern using HEREDOC for multi-line content
- `/workspace/artifacts/workflow-design/WORKFLOW.md` lines 142-179 - bd comments as primary data transport between stages
- `/workspace/artifacts/lessons-learned.md` lines 11-12 - Verify toolset includes Bash before using bd commands

## Files to change
- [ ] `/workspace/agents/reviewer.md` - Add Phase 5.5 "Emit Metrics Payload" between Phase 5 and Phase 6
  - Create JSON payload with schema v1, timestamp, task IDs, severity counts, category breakdown, issues array
  - Emit via `bd comments add .claude-metrics "[JSON]"`
  - Handle zero-issue reviews (all counts = 0)
- [ ] `/workspace/artifacts/workflow-design/WORKFLOW.md` - Add documentation about metrics tracking
  - Add subsection under "Stage Outputs" explaining metrics collection
  - Document the `.claude-metrics` task as a special persistent data store

## Watch out for
- **JSON escaping in bd comments**: Use HEREDOC pattern (`cat <<'EOF'`) to avoid shell quoting issues with nested quotes in JSON
- **Schema version field**: Start at v1 and include in every payload - this allows future schema evolution without breaking consumers
- **Category naming convention**: Use snake_case (testing, ui_ux, business_logic) not camelCase or kebab-case - spec is explicit about this
- **Empty categories object**: Only include categories with count > 0, but always include all three severity levels even if 0
- **Timestamp format**: Must be ISO 8601 with timezone (`date -u +"%Y-%m-%dT%H:%M:%SZ"` in bash)
- **Task ID confusion**: Payload includes both `task` (reviewer's own subtask like `.claude-gvl.4`) and `parent` (the task being reviewed like `.claude-gvl`)

## Dependencies to be careful with
- `.claude-metrics` task must exist before reviewer can emit payloads - this is a **one-time manual setup**, not automated
- Reviewer already has Bash in toolset (line 5 of reviewer.md), so `bd comments add` is safe to use
- bd comments have unknown size limits - if payload is too large, prioritize severity/category counts over the detailed issues array

## Testing approach
- Unit: No traditional unit tests for agent markdown files
- Integration: Create a test review scenario and verify:
  1. Reviewer emits valid JSON after review
  2. JSON conforms to schema (all required fields present)
  3. `bd comments .claude-metrics` returns parseable JSON
  4. Zero-issue review produces valid payload with counts = 0
  5. Multiple concurrent reviews don't conflict (append-only pattern)
- Edge cases from spec:
  - Zero issues: Verify severity all 0, categories empty object, issues empty array
  - New category: Verify reviewer can add custom category in snake_case
  - Very long review: Check if issues array can handle 20+ issues without hitting limits

## Documentation to update
- [ ] `/workspace/artifacts/workflow-design/WORKFLOW.md` - Add metrics collection documentation
  - Under "Stage Outputs", add section explaining reviewer emits metrics
  - Document `.claude-metrics` as special persistent task
  - Explain append-only pattern for concurrent safety
- [ ] `/workspace/agents/reviewer.md` - Add Phase 5.5 with clear examples
  - Show complete JSON structure with all fields
  - Include example for zero-issue review
  - Note when reviewer should add new categories
  - Document the 9 base categories with their meanings

## Pre-implementation setup
The implementer must handle the one-time setup of the metrics task:
```bash
# Check if .claude-metrics already exists
bd list | grep claude-metrics

# If not, create it:
bd create "Review metrics tracking" --id .claude-metrics -d "Persistent storage for reviewer metrics. Each comment is a JSON payload from a review."
```

This task should remain open indefinitely - it's a data store, not a work item.

## Implementation sequence
1. **Setup**: Verify `.claude-metrics` task exists (create if needed)
2. **Reviewer agent changes**: Add Phase 5.5 to emit JSON payload
   - Parse review findings into severity counts
   - Categorize issues (use existing 9 categories, add new if needed)
   - Build JSON structure matching schema
   - Emit via bd comments with proper escaping
3. **Documentation updates**: Update WORKFLOW.md with metrics explanation
4. **Testing**: Manually test with a mock review to verify JSON validity

## Category mapping guidance for reviewer
The reviewer should map issues to these base categories (from spec):
- `testing` - Missing/inadequate tests
- `ui_ux` - Frontend/interface issues  
- `business_logic` - Incorrect behavior
- `security` - Vulnerabilities
- `performance` - Efficiency issues
- `code_quality` - Structure, naming, patterns
- `documentation` - Missing/incorrect docs
- `hints` - Missing hints/guidance
- `configuration` - Config issues

If an issue doesn't fit, the reviewer adds a new snake_case category and documents why in the issue summary.

## JSON structure example
```json
{
  "v": 1,
  "ts": "2026-01-31T20:30:00Z",
  "task": ".claude-gvl.4",
  "parent": ".claude-gvl",
  "severity": {
    "critical": 0,
    "major": 2,
    "minor": 3
  },
  "categories": {
    "testing": 1,
    "business_logic": 2,
    "code_quality": 2
  },
  "issues": [
    {
      "severity": "major",
      "category": "testing",
      "file": "src/auth.ts",
      "line": 42,
      "summary": "Missing error case test"
    }
  ]
}
```

## Lessons from past work
From `/workspace/artifacts/lessons-learned.md`:
- **Toolset verification** (lines 11-12): Reviewer already has Bash in toolset (line 5 of reviewer.md), so we're safe to add `bd comments add` commands
- **HEREDOC pattern** (implied from existing agents): Use `cat <<'EOF'` to avoid shell escaping issues with JSON quotes - this is already the pattern in reviewer.md lines 103-113

## Out of scope (per spec)
- Aggregation/reporting logic (separate tool handles this)
- Visualization (separate tool)
- Automatic `.claude-metrics` task creation (manual setup is acceptable)
- Category standardization enforcement (reviewer uses judgment)
- Historical data migration (start fresh)

## Acceptance criteria
- [ ] `.claude-metrics` task exists and documented
- [ ] Reviewer agent emits valid JSON after every review
- [ ] JSON payload conforms to schema (v, ts, task, parent, severity, categories all present)
- [ ] Zero-issue reviews produce valid datapoints (all counts = 0)
- [ ] `bd comments .claude-metrics` returns parseable JSON entries
- [ ] Documentation updated in WORKFLOW.md and reviewer.md
