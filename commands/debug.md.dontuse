---
description: Generate debugging hypotheses for unexpected behavior
argument-hint: "<description of unexpected behavior, error messages, stack traces>"
---

# Debug Hypothesis Generator

You analyze unexpected behavior and produce structured debugging hypotheses. Your goal is to save debugging time by identifying the most likely causes and providing concrete ways to test each one.

## Input

A description of unexpected behavior, which may include:
- What was expected vs. what actually happened
- Error messages or stack traces
- Recent changes that might be related
- Environmental context (OS, versions, etc.)

$ARGUMENTS

## Phase 1: Parse the Problem

Extract and organize the key information:

1. **Symptom**: What exactly is happening? (observable behavior)
2. **Expected**: What should happen instead?
3. **Context**: When/where does it occur? (always, sometimes, specific conditions)
4. **Error signals**: Any error messages, codes, or stack traces
5. **Timeline**: When did it start? What changed recently?

If critical information is missing, note what would help narrow down the cause.

## Phase 2: Generate Hypotheses

Produce 3-5 hypotheses ranked by likelihood. For each hypothesis:

```markdown
### Hypothesis [N]: [Brief title]

**Likelihood:** [High|Medium|Low]

**Theory:** [1-2 sentences explaining the suspected cause]

**Supporting evidence:**
- [What in the problem description points to this]

**Contradicting evidence:**
- [What argues against this, if anything]

**How to test:**
1. [Specific, actionable step to confirm or rule out]
2. [Additional verification if needed]

**If confirmed, offer to:**
- fix it
- create a task using /bd-task
```

### Hypothesis Categories to Consider

Work through these systematically:

| Category | Common Causes |
|----------|---------------|
| **State** | Stale cache, corrupted state, race condition, uninitialized variable |
| **Environment** | Wrong version, missing dependency, env var, permissions, path issues |
| **Data** | Invalid input, encoding, null/undefined, type mismatch, boundary case |
| **Timing** | Race condition, timeout, async ordering, deadlock |
| **Integration** | API change, contract mismatch, network issue, auth/token expiry |
| **Resource** | Memory, disk, connections, file handles, rate limits |
| **Configuration** | Wrong config loaded, precedence issue, typo in config key |
| **Code path** | Wrong branch taken, early return, exception swallowed |

### Prioritization Factors

Rank hypotheses higher when:
- They explain ALL observed symptoms
- Recent changes could have introduced them
- They're common failure modes for this type of system
- Quick to test (prefer ruling out fast checks first)

Rank lower when:
- They only explain partial symptoms
- They require unlikely coincidences
- The area hasn't been touched recently

## Phase 3: Suggest Investigation Order

Provide a recommended debugging sequence:

```markdown
## Recommended Investigation Order

1. **[Hypothesis N]** - [Why first: quick to test / most likely / etc.]
2. **[Hypothesis M]** - [Why next]
...

### Quick Wins to Try First
- [ ] [Simple check that rules out multiple hypotheses]
- [ ] [Another fast diagnostic]

### If All Hypotheses Fail
- [Suggestions for gathering more information]
- [Alternative diagnostic approaches]
```

## Phase 4: Offer Assistance

After presenting hypotheses, offer to help with:
- Exploring specific code paths related to top hypotheses
- Writing diagnostic code or log statements
- Searching the codebase for similar patterns or past fixes
- Testing specific hypotheses by reading relevant files

## Output Format

```markdown
# Debug Analysis: [Brief problem summary]

## Problem Summary
[Parsed understanding of the issue]

## Missing Information
[What additional context would help, if any]

## Hypotheses

### Hypothesis 1: [Title]
[Full hypothesis block]

### Hypothesis 2: [Title]
[Full hypothesis block]

[... more hypotheses ...]

## Recommended Investigation Order
[Prioritized list with reasoning]

## Quick Wins
[Fast checks to try first]

## Need More Info?
[What to do if hypotheses don't pan out]

---
Ready to help investigate any of these. Which hypothesis should we explore first?
```

## Rules

- **Be specific** - vague hypotheses waste time. "Something is wrong with auth" is useless; "JWT token expiry check uses server time but token was issued with client time" is actionable.
- **Test the obvious** - include "did you try turning it off and on" level checks if relevant. Caches, restarts, and re-installs fix more bugs than we like to admit.
- **Consider recency** - recent changes are disproportionately likely to be the cause.
- **Match the evidence** - a hypothesis must explain the observed symptoms, not just be theoretically possible.
- **Provide escape hatches** - if none of the hypotheses pan out, give guidance on what to try next.
- **Stay practical** - focus on hypotheses that can actually be tested with available tools and access.
